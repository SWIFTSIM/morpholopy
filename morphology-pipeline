#!/usr/bin/env python3
"""
morphology-pipeline is used to generate complex morphological plots for one or
multiple COLIBRE simulation runs. These plot require a relatively expensive
analysis of galaxies in the simulation on an individual galaxy basis and are
therefore more challenging to generate than the plots in the conventional
pipeline.

ADD MORE DESCRIPTION HERE
"""

import argparse as ap
import multiprocessing as mp
import os

parser = ap.ArgumentParser(
    prog="morphology-pipeline",
    description=("TO PROVIDE"),
    epilog=("Example usage: TO PROVIDE\n"),
)

parser.add_argument(
    "-C",
    "--config",
    type=str,
    required=True,
    help=("Configuration directory, containing config.yml."),
)

parser.add_argument(
    "-c",
    "--catalogues",
    type=str,
    required=True,
    help="Name of the VELOCIraptor HDF5 .properties file(s). Required.",
    nargs="*",
)

parser.add_argument(
    "-s",
    "--snapshots",
    required=True,
    type=str,
    help="Name of the snapshot file(s). Required.",
    nargs="*",
)

parser.add_argument(
    "-o",
    "--output",
    type=str,
    required=True,
    help="Output directory for figures. Required.",
)

parser.add_argument(
    "-i",
    "--input",
    type=str,
    required=False,
    default=".",
    help=(
        "Input directory where the snapshot(s) and properties file(s) are located. "
        "Default is the current working directory. If you are running for comparison "
        "purposes you will need to ensure that the metadata yaml files have been "
        "generated in these folders and have the same basename (--metadata) as is "
        "given here."
    ),
    nargs="*",
)

# debug mode?
"""
parser.add_argument(
    "-d",
    "--debug",
    required=False,
    default=False,
    action="store_true",
    help="Run in debug mode if this flag is present. Default: no.",
)
"""


parser.add_argument(
    "-m",
    "--metadata",
    required=False,
    default="morphology_data",
    help=(
        "Base name of the written metadata file in the input directory. "
        "By default this is morphology_data, leading to morphology_data_XXXX.yml"
    ),
)

parser.add_argument(
    "-n",
    "--run-names",
    required=False,
    default=None,
    nargs="*",
    help=(
        "Overwrite the names given to each run? If not present, the default names "
        "from the snapshots are used, and in the case where there are multiple "
        "redshifts, we append the redshift."
    ),
)

parser.add_argument(
    "-j",
    "--num-of-cpus",
    required=False,
    type=int,
    default=None,
    help=(
        "Number of CPUs to use for running scripts in parallel. If not specified, uses "
        "the maximum number of CPUs avaliable in the system."
    ),
)


def init_child_process(env, temp_folder):
    """
    Matplotlib with LaTeX support is not safe to use in parallel.
    The reason is that Matplotlib saves some intermediary files in a temporary
    directory (MPLCONFIGDIR environment variable) in order to post-process
    figures that use LaTeX. All these temporary files end up in the same
    directory and apparently can have the same names when you run multiple
    Python processes that try to save a figure at the same time.
    The only way around this is to use a different temporary directory for each
    subprocess, which is what this function does: it is run exactly once when
    the subprocess is created and sets the MPLCONFIGDIR environment variable
    for this subprocess to a unique value.
    """
    print(mp.current_process().pid, "starting...")
    env["MPLCONFIGDIR"] = f"{temp_folder}/temp_folder_pid_{mp.current_process().pid}"
    os.environ = env


if __name__ == "__main__":

    from velociraptor import load as load_catalogue
    from swiftsimio import load as load_snapshot

    from morpholopy.filtered_catalogue import FilteredCatalogue
    from morpholopy.galaxy_data import process_galaxy, AllGalaxyData

    from morpholopy.HI_size import plot_HI_size_mass

    from morpholopy.config import MorphologyConfig
    from swiftpipeline.html import WebpageCreator

    import matplotlib

    matplotlib.use("Agg")
    import matplotlib.pyplot as pl

    import unyt
    import os
    import tempfile
    import shutil

    # minimise the memory footprint of parallel processes by ensuring only
    # relevant data in memory are copied
    mp.set_start_method("forkserver")

    # create a temporary directory for the per subprocess Matplotlib cache
    # directories
    tmpdir = tempfile.mkdtemp()
    print(f"Will save temporary files in {tmpdir}")

    args = parser.parse_args()

    config = MorphologyConfig(config_directory=args.config)

    if config.matplotlib_stylesheet != "default":
        stylesheet_path = f"{config.config_directory}/{config.matplotlib_stylesheet}"
        pl.style.use(stylesheet_path)

    snapshots = [
        load_snapshot(f"{input}/{snapshot}")
        for input, snapshot in zip(args.input, args.snapshots)
    ]
    if args.run_names is not None:
        run_names = args.run_names
    else:
        # First, check if the snapshots are all at the same redshift
        redshifts = {data.metadata.redshift for data in snapshots}
        # If the size of the set is one, then all redshifts are the same
        if len(redshifts) == 1:
            # All redshifts are the same! No need to modify runs' names
            run_names = [data.metadata.run_name for data in snapshots]
        # If the size of the set > 1, then at least two runs have different redshifts
        else:
            # Need to append appropriate redshifts to names.
            run_names = [
                f"{data.metadata.run_name} (z={data.metadata.redshift:1.3f})"
                for data in snapshots
            ]

    # observational data will be added later
    """
    observational_data_path = (
        f"{config.config_directory}/{config.observational_data_directory}/data"
    )
    """

    is_comparison = len(args.snapshots) > 1

    all_galaxies_list = []
    if not is_comparison:
        halo_catalogue_filename = f"{args.input[0]}/{args.catalogues[0]}"
        snapshot_filename = f"{args.input[0]}/{args.snapshots[0]}"

        catalogue = load_catalogue(halo_catalogue_filename, disregard_units=True)
        filtered_catalogue = FilteredCatalogue(
            catalogue,
            config.mass_limit_stars_in_Msun,
            config.mass_variable_stars,
            config.mass_limit_gas_in_Msun,
            config.mass_variable_gas,
            config.plotting_lower_mass_limit_in_Msun,
            config.plotting_upper_mass_limit_in_Msun,
            config.plotting_number_of_galaxies,
            config.plotting_random_seed,
        )
        # close the catalogue by unloading it explicitly
        catalogue = None

        # create an empty object to store morphological data for all galaxies
        number_of_galaxies = len(filtered_catalogue.galaxy_indices)
        all_galaxies = AllGalaxyData(number_of_galaxies)

        # create a list of arguments for parallel processing
        arglist = [
            (
                index,
                galaxy_index,
                halo_catalogue_filename,
                snapshot_filename,
                args.output,
                config.orientation_method,
                make_plots,
            )
            for index, (galaxy_index, make_plots) in enumerate(
                zip(filtered_catalogue.galaxy_indices, filtered_catalogue.plot_galaxy)
            )
        ]

        # determine the appropriate number of parallel processes to use
        num_proc = args.num_of_cpus
        if num_proc is None:
            num_proc = mp.cpu_count()
        # make sure we don't use more processes than there are galaxies
        num_proc = min(num_proc, number_of_galaxies)
        """
        # serial version (has better crash reports)
        for index, galaxy_data in map(process_galaxy, arglist):
        """
        parent_env = os.environ.copy()
        pool = mp.Pool(
            num_proc, initializer=init_child_process, initargs=(parent_env, tmpdir)
        )
        for index, galaxy_data in pool.imap_unordered(process_galaxy, arglist):
            # add galaxy contribution to global data
            all_galaxies[index] = galaxy_data

        # save plot data using the given metadata name
        metadata_filename = (
            f"{args.input[0]}/{args.metadata}_{args.snapshots[0][-9:-5]}.yml"
        )
        all_galaxies.output(metadata_filename)

        # regenerate the data from the file, for consistency with the comparison case
        all_galaxies_list = [AllGalaxyData.fromfile(metadata_filename)]
    else:
        # Need to generate our data again from the built-in yaml files.
        metadata_filenames = [
            f"{input}/{args.metadata}_{snapshot[-9:-5]}.yml"
            for input, snapshot in zip(args.input, args.snapshots)
        ]

        # recreate data from metadata files
        all_galaxies_list = [
            AllGalaxyData.fromfile(filename) for filename in metadata_filenames
        ]

    if not os.path.exists(args.output):
        os.mkdir(args.output)

    plot_HI_size_mass(args.output, run_names, all_galaxies_list)

    # Create the webpage
    webpage = WebpageCreator()
    webpage.add_config_metadata(config=config, is_comparison=is_comparison)
    webpage.add_metadata(page_name=" | ".join(run_names))
    webpage.add_run_metadata(config=config, snapshots=snapshots)
    webpage.render_webpage()
    webpage.save_html(f"{args.output}/index.html")

    # Remove the temporary directory hosting the Matplotlib caches
    print(f"Removing temporary directory {tmpdir}")
    shutil.rmtree(tmpdir)
